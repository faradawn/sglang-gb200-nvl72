+ echo '=== 11-30 low precision prefill SLURM_NODEID 12'
=== 11-30 low precision prefill SLURM_NODEID 12
+ export SGLANG_HEALTH_CHECK_TIMEOUT=3600
+ SGLANG_HEALTH_CHECK_TIMEOUT=3600
+ [[ 12 -lt 1 ]]
+ export TORCH_DISTRIBUTED_DEFAULT_TIMEOUT=1800
+ TORCH_DISTRIBUTED_DEFAULT_TIMEOUT=1800
+ node_rank=11
+ export SGLANG_TORCH_PROFILER_DIR=/base_dir/../decode_torch_profiler
+ SGLANG_TORCH_PROFILER_DIR=/base_dir/../decode_torch_profiler
+ mkdir -p /base_dir/../decode_torch_profiler
+ export SGLANG_DUMPER_DIR=/base_dir/../decode_sglang_dump
+ SGLANG_DUMPER_DIR=/base_dir/../decode_sglang_dump
+ mkdir -p /base_dir/../decode_sglang_dump
+ export SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/base_dir/../decode_sglang_expert_distribution_recorder
+ SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/base_dir/../decode_sglang_expert_distribution_recorder
+ mkdir -p /base_dir/../decode_sglang_expert_distribution_recorder
+ export FLASHINFER_WORKSPACE_BASE=/base_dir/../decode_flashinfer_workspace_base
+ FLASHINFER_WORKSPACE_BASE=/base_dir/../decode_flashinfer_workspace_base
+ mkdir -p /base_dir/../decode_flashinfer_workspace_base
+ export SGL_DG_CACHE_DIR=/base_dir/../decode_deepgemm_cache
+ SGL_DG_CACHE_DIR=/base_dir/../decode_deepgemm_cache
+ mkdir -p /base_dir/../decode_deepgemm_cache
+ SERVER_PID=180770
+ DYN_SKIP_SGLANG_LOG_FORMATTING=1
+ [[ 12 -eq 0 ]]
+ SGLANG_NVFP4_CKPT_FP8_GEMM_IN_ATTN=1
+ wait 180770
+ SGLANG_PER_TOKEN_GROUP_QUANT_8BIT_V2=1
+ SGL_JIT_DEEPGEMM_PRECOMPILE=0
+ SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=1024
+ SGLANG_CUTEDSL_MOE_NVFP4_DISPATCH=1
+ SGLANG_FP4_GEMM_BACKEND=cutlass
+ MC_TE_METRIC=true
+ SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE=100000
+ SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT=100000
+ SGLANG_DISAGGREGATION_WAITING_TIMEOUT=100000
+ SGLANG_HACK_SEQ_BOOTSTRAP_ROOM=1
+ SGLANG_MOONCAKE_CUSTOM_MEM_POOL=True
+ MC_FORCE_MNNVL=1
+ NCCL_MNNVL_ENABLE=1
+ NCCL_CUMEM_ENABLE=1
+ SGLANG_USE_MESSAGE_QUEUE_BROADCASTER=0
+ SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK=1
+ PYTHONUNBUFFERED=1
+ python3 -m sglang.launch_server --model-path nvidia/DeepSeek-R1-0528-FP4 --trust-remote-code --disaggregation-mode decode --dist-init-addr 10.52.103.108:20000 --dist-timeout 3600 --disaggregation-bootstrap-port 30001 --nnodes 12 --node-rank 11 --tp-size 48 --dp-size 48 --ep-size 48 --enable-dp-attention --host 0.0.0.0 --port 30000 --decode-log-interval 1 --max-running-requests 49152 --context-length 4224 --disable-radix-cache --disable-shared-experts-fusion --watchdog-timeout 1000000 --disable-chunked-prefix-cache --kv-cache-dtype fp8_e4m3 --enable-single-batch-overlap --mem-fraction-static 0.83 --moe-a2a-backend deepep --deepep-mode low_latency --ep-dispatch-algorithm static --cuda-graph-bs 1024 --num-reserved-decode-tokens 112 --ep-num-redundant-experts 32 --eplb-algorithm deepseek --moe-dense-tp-size 1 --enable-dp-lm-head --prefill-round-robin-balance --max-total-tokens 3122380 --max-prefill-tokens 16384 --quantization modelopt_fp4 --moe-runner-backend flashinfer_cutedsl
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_JIT_DEEPGEMM_PRECOMPILE is deprecated, please use SGLANG_JIT_DEEPGEMM_PRECOMPILE
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_DG_CACHE_DIR is deprecated, please use SGLANG_DG_CACHE_DIR
  warnings.warn(
[2025-11-30 09:15:54] INFO server_args.py:941: Use trtllm_mla as attention backend on sm100 for DeepseekV3ForCausalLM
[2025-11-30 09:15:55] WARNING server_args.py:1329: TensorRT-LLM MLA only supports page_size of 32 or 64, changing page_size from None to 64.
[2025-11-30 09:15:55] WARNING server_args.py:1453: DP attention is enabled. The chunked prefill size is adjusted to 341 to avoid MoE kernel issues. 
[2025-11-30 09:15:55] WARNING server_args.py:1502: DeepEP MoE is enabled. The expert parallel size is adjusted to be the same as the tensor parallel size[48].
[2025-11-30 09:15:55] WARNING server_args.py:1770: KV cache is forced as chunk cache for decode server
[2025-11-30 09:15:55] server_args=ServerArgs(model_path='nvidia/DeepSeek-R1-0528-FP4', tokenizer_path='nvidia/DeepSeek-R1-0528-FP4', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=4224, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelopt_fp4', quantization_param_path=None, kv_cache_dtype='fp8_e4m3', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.83, max_running_requests=49152, max_queued_requests=None, max_total_tokens=3122380, chunked_prefill_size=341, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=64, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=48, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=164856224, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=1000000.0, dist_timeout=3600, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=1, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='nvidia/DeepSeek-R1-0528-FP4', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=48, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=True, dist_init_addr='10.52.103.108:20000', nnodes=12, node_rank=11, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='trtllm_mla', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=48, moe_a2a_backend='deepep', moe_runner_backend='flashinfer_cutedsl', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='low_latency', ep_num_redundant_experts=32, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='deepseek', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=1, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=1024, cuda_graph_bs=[1024], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=True, enable_two_batch_overlap=False, enable_single_batch_overlap=True, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=True, disable_chunked_prefix_cache=True, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=30001, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=112, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, hooks=None)
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_JIT_DEEPGEMM_PRECOMPILE is deprecated, please use SGLANG_JIT_DEEPGEMM_PRECOMPILE
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_DG_CACHE_DIR is deprecated, please use SGLANG_DG_CACHE_DIR
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_JIT_DEEPGEMM_PRECOMPILE is deprecated, please use SGLANG_JIT_DEEPGEMM_PRECOMPILE
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_DG_CACHE_DIR is deprecated, please use SGLANG_DG_CACHE_DIR
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_JIT_DEEPGEMM_PRECOMPILE is deprecated, please use SGLANG_JIT_DEEPGEMM_PRECOMPILE
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_DG_CACHE_DIR is deprecated, please use SGLANG_DG_CACHE_DIR
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_JIT_DEEPGEMM_PRECOMPILE is deprecated, please use SGLANG_JIT_DEEPGEMM_PRECOMPILE
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_DG_CACHE_DIR is deprecated, please use SGLANG_DG_CACHE_DIR
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_JIT_DEEPGEMM_PRECOMPILE is deprecated, please use SGLANG_JIT_DEEPGEMM_PRECOMPILE
  warnings.warn(
/sgl-workspace/sglang/python/sglang/srt/environ.py:337: UserWarning: Environment variable SGL_DG_CACHE_DIR is deprecated, please use SGLANG_DG_CACHE_DIR
  warnings.warn(
[2025-11-30 09:16:09 DP44 TP44 EP44] Init torch distributed begin.
[2025-11-30 09:16:09 DP47 TP47 EP47] Init torch distributed begin.
[2025-11-30 09:16:09 DP46 TP46 EP46] Init torch distributed begin.
[2025-11-30 09:16:10 DP45 TP45 EP45] Init torch distributed begin.
[Gloo] Rank [Gloo] Rank [Gloo] Rank [Gloo] Rank 44454746 is connected to  is connected to  is connected to  is connected to 47474747 peer ranks.  peer ranks.  peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : Expected number of connected peer ranks is : Expected number of connected peer ranks is : 47474747



[Gloo] Rank [Gloo] Rank [Gloo] Rank 4746[Gloo] Rank 45 is connected to  is connected to 44 is connected to 4747 is connected to 47 peer ranks.  peer ranks. 47 peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is :  peer ranks. Expected number of connected peer ranks is : 4747Expected number of connected peer ranks is : 47

47

[2025-11-30 09:16:13 DP45 TP45 EP45] Custom allreduce is disabled because this process group spans across nodes.
[2025-11-30 09:16:13 DP44 TP44 EP44] Custom allreduce is disabled because this process group spans across nodes.
[2025-11-30 09:16:13 DP46 TP46 EP46] Custom allreduce is disabled because this process group spans across nodes.
[2025-11-30 09:16:13 DP47 TP47 EP47] Custom allreduce is disabled because this process group spans across nodes.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-11-30 09:16:13 DP47 TP47 EP47] Init torch distributed ends. mem usage=1.07 GB
[2025-11-30 09:16:13 DP46 TP46 EP46] Init torch distributed ends. mem usage=1.07 GB
[2025-11-30 09:16:13 DP44 TP44 EP44] Init torch distributed ends. mem usage=1.07 GB
[2025-11-30 09:16:13 DP45 TP45 EP45] Init torch distributed ends. mem usage=1.07 GB
[2025-11-30 09:16:14 DP47 TP47 EP47] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-30 09:16:14 DP44 TP44 EP44] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-30 09:16:14 DP45 TP45 EP45] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-30 09:16:14 DP46 TP46 EP46] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-30 09:16:18 DP44 TP44 EP44] Load weight begin. avail mem=182.18 GB
[2025-11-30 09:16:18 DP44 TP44 EP44] Using ModelOptModelLoader due to ModelOpt quantization config.
[2025-11-30 09:16:18 DP44 TP44 EP44] ModelOptModelLoader: Loading base model...
[2025-11-30 09:16:18 DP45 TP45 EP45] Load weight begin. avail mem=182.19 GB
[2025-11-30 09:16:18 DP45 TP45 EP45] Using ModelOptModelLoader due to ModelOpt quantization config.
[2025-11-30 09:16:18 DP45 TP45 EP45] ModelOptModelLoader: Loading base model...
[2025-11-30 09:16:18 DP47 TP47 EP47] Load weight begin. avail mem=182.19 GB
[2025-11-30 09:16:18 DP47 TP47 EP47] Using ModelOptModelLoader due to ModelOpt quantization config.
[2025-11-30 09:16:18 DP47 TP47 EP47] ModelOptModelLoader: Loading base model...
[2025-11-30 09:16:18 DP46 TP46 EP46] Load weight begin. avail mem=182.19 GB
[2025-11-30 09:16:18 DP46 TP46 EP46] Using ModelOptModelLoader due to ModelOpt quantization config.
[2025-11-30 09:16:18 DP46 TP46 EP46] ModelOptModelLoader: Loading base model...
[2025-11-30 09:16:18 DP44 TP44 EP44] Model is already quantized, loading directly...
[2025-11-30 09:16:18 DP45 TP45 EP45] Model is already quantized, loading directly...
[2025-11-30 09:16:18 DP47 TP47 EP47] Model is already quantized, loading directly...
[2025-11-30 09:16:18 DP46 TP46 EP46] Model is already quantized, loading directly...
[2025-11-30 09:16:18 DP44 TP44 EP44] Detected nvfp4 checkpoint. Please note that the format is experimental and subject to change.
[2025-11-30 09:16:18 DP46 TP46 EP46] Detected nvfp4 checkpoint. Please note that the format is experimental and subject to change.
[2025-11-30 09:16:19 DP47 TP47 EP47] Detected nvfp4 checkpoint. Please note that the format is experimental and subject to change.
[2025-11-30 09:16:19 DP45 TP45 EP45] Detected nvfp4 checkpoint. Please note that the format is experimental and subject to change.
quant attn to fp8 ue8m0:   0%|          | 0/61 [00:00<?, ?it/s]quant attn to fp8 ue8m0:   0%|          | 0/61 [00:00<?, ?it/s]quant attn to fp8 ue8m0:   0%|          | 0/61 [00:00<?, ?it/s]quant attn to fp8 ue8m0:   0%|          | 0/61 [00:00<?, ?it/s]quant attn to fp8 ue8m0:   2%|▏         | 1/61 [00:00<00:06,  9.27it/s]quant attn to fp8 ue8m0:   2%|▏         | 1/61 [00:00<00:06,  9.26it/s]quant attn to fp8 ue8m0:   2%|▏         | 1/61 [00:00<00:06,  9.21it/s]quant attn to fp8 ue8m0:   2%|▏         | 1/61 [00:00<00:06,  9.18it/s]quant attn to fp8 ue8m0:   3%|▎         | 2/61 [00:00<00:06,  9.01it/s]quant attn to fp8 ue8m0:   3%|▎         | 2/61 [00:00<00:06,  8.98it/s]quant attn to fp8 ue8m0:   3%|▎         | 2/61 [00:00<00:06,  8.98it/s]quant attn to fp8 ue8m0:   3%|▎         | 2/61 [00:00<00:06,  8.97it/s]quant attn to fp8 ue8m0:   5%|▍         | 3/61 [00:00<00:06,  8.62it/s]quant attn to fp8 ue8m0:   5%|▍         | 3/61 [00:00<00:06,  8.62it/s]quant attn to fp8 ue8m0:   5%|▍         | 3/61 [00:00<00:06,  8.61it/s]quant attn to fp8 ue8m0:   5%|▍         | 3/61 [00:00<00:06,  8.60it/s]quant attn to fp8 ue8m0:   7%|▋         | 4/61 [00:00<00:06,  8.82it/s]quant attn to fp8 ue8m0:   7%|▋         | 4/61 [00:00<00:06,  8.82it/s]quant attn to fp8 ue8m0:   7%|▋         | 4/61 [00:00<00:06,  8.81it/s]quant attn to fp8 ue8m0:   7%|▋         | 4/61 [00:00<00:06,  8.79it/s]quant attn to fp8 ue8m0:   8%|▊         | 5/61 [00:00<00:06,  8.60it/s]quant attn to fp8 ue8m0:   8%|▊         | 5/61 [00:00<00:06,  8.60it/s]quant attn to fp8 ue8m0:   8%|▊         | 5/61 [00:00<00:06,  8.60it/s]quant attn to fp8 ue8m0:   8%|▊         | 5/61 [00:00<00:06,  8.59it/s]quant attn to fp8 ue8m0:  10%|▉         | 6/61 [00:00<00:06,  8.64it/s]quant attn to fp8 ue8m0:  10%|▉         | 6/61 [00:00<00:06,  8.63it/s]quant attn to fp8 ue8m0:  10%|▉         | 6/61 [00:00<00:06,  8.63it/s]quant attn to fp8 ue8m0:  10%|▉         | 6/61 [00:00<00:06,  8.62it/s]quant attn to fp8 ue8m0:  11%|█▏        | 7/61 [00:00<00:06,  8.75it/s]quant attn to fp8 ue8m0:  11%|█▏        | 7/61 [00:00<00:06,  8.75it/s]quant attn to fp8 ue8m0:  11%|█▏        | 7/61 [00:00<00:06,  8.73it/s]quant attn to fp8 ue8m0:  11%|█▏        | 7/61 [00:00<00:06,  8.73it/s]quant attn to fp8 ue8m0:  13%|█▎        | 8/61 [00:00<00:06,  8.51it/s]quant attn to fp8 ue8m0:  13%|█▎        | 8/61 [00:00<00:06,  8.51it/s]quant attn to fp8 ue8m0:  13%|█▎        | 8/61 [00:00<00:06,  8.51it/s]quant attn to fp8 ue8m0:  13%|█▎        | 8/61 [00:00<00:06,  8.50it/s]quant attn to fp8 ue8m0:  15%|█▍        | 9/61 [00:01<00:06,  8.33it/s]quant attn to fp8 ue8m0:  15%|█▍        | 9/61 [00:01<00:06,  8.33it/s]quant attn to fp8 ue8m0:  15%|█▍        | 9/61 [00:01<00:06,  8.32it/s]quant attn to fp8 ue8m0:  15%|█▍        | 9/61 [00:01<00:06,  8.32it/s]quant attn to fp8 ue8m0:  16%|█▋        | 10/61 [00:01<00:06,  8.40it/s]quant attn to fp8 ue8m0:  16%|█▋        | 10/61 [00:01<00:06,  8.40it/s]quant attn to fp8 ue8m0:  16%|█▋        | 10/61 [00:01<00:06,  8.39it/s]quant attn to fp8 ue8m0:  16%|█▋        | 10/61 [00:01<00:06,  8.39it/s]quant attn to fp8 ue8m0:  18%|█▊        | 11/61 [00:01<00:05,  8.45it/s]quant attn to fp8 ue8m0:  18%|█▊        | 11/61 [00:01<00:05,  8.44it/s]quant attn to fp8 ue8m0:  18%|█▊        | 11/61 [00:01<00:05,  8.43it/s]quant attn to fp8 ue8m0:  18%|█▊        | 11/61 [00:01<00:05,  8.43it/s]quant attn to fp8 ue8m0:  20%|█▉        | 12/61 [00:01<00:05,  8.62it/s]quant attn to fp8 ue8m0:  20%|█▉        | 12/61 [00:01<00:05,  8.60it/s]quant attn to fp8 ue8m0:  20%|█▉        | 12/61 [00:01<00:05,  8.61it/s]quant attn to fp8 ue8m0:  20%|█▉        | 12/61 [00:01<00:05,  8.60it/s]quant attn to fp8 ue8m0:  21%|██▏       | 13/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  21%|██▏       | 13/61 [00:01<00:05,  8.36it/s]quant attn to fp8 ue8m0:  21%|██▏       | 13/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  21%|██▏       | 13/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  23%|██▎       | 14/61 [00:01<00:05,  8.36it/s]quant attn to fp8 ue8m0:  23%|██▎       | 14/61 [00:01<00:05,  8.36it/s]quant attn to fp8 ue8m0:  23%|██▎       | 14/61 [00:01<00:05,  8.36it/s]quant attn to fp8 ue8m0:  23%|██▎       | 14/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  25%|██▍       | 15/61 [00:01<00:05,  8.57it/s]quant attn to fp8 ue8m0:  25%|██▍       | 15/61 [00:01<00:05,  8.58it/s]quant attn to fp8 ue8m0:  25%|██▍       | 15/61 [00:01<00:05,  8.57it/s]quant attn to fp8 ue8m0:  25%|██▍       | 15/61 [00:01<00:05,  8.56it/s]quant attn to fp8 ue8m0:  26%|██▌       | 16/61 [00:01<00:05,  8.36it/s]quant attn to fp8 ue8m0:  26%|██▌       | 16/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  26%|██▌       | 16/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  26%|██▌       | 16/61 [00:01<00:05,  8.35it/s]quant attn to fp8 ue8m0:  28%|██▊       | 17/61 [00:01<00:05,  8.37it/s]quant attn to fp8 ue8m0:  28%|██▊       | 17/61 [00:01<00:05,  8.38it/s]quant attn to fp8 ue8m0:  28%|██▊       | 17/61 [00:01<00:05,  8.37it/s]quant attn to fp8 ue8m0:  28%|██▊       | 17/61 [00:01<00:05,  8.37it/s]quant attn to fp8 ue8m0:  30%|██▉       | 18/61 [00:02<00:05,  8.33it/s]quant attn to fp8 ue8m0:  30%|██▉       | 18/61 [00:02<00:05,  8.33it/s]quant attn to fp8 ue8m0:  30%|██▉       | 18/61 [00:02<00:05,  8.33it/s]quant attn to fp8 ue8m0:  30%|██▉       | 18/61 [00:02<00:05,  8.33it/s]quant attn to fp8 ue8m0:  31%|███       | 19/61 [00:02<00:05,  8.29it/s]quant attn to fp8 ue8m0:  31%|███       | 19/61 [00:02<00:05,  8.30it/s]quant attn to fp8 ue8m0:  31%|███       | 19/61 [00:02<00:05,  8.29it/s]quant attn to fp8 ue8m0:  31%|███       | 19/61 [00:02<00:05,  8.29it/s]quant attn to fp8 ue8m0:  33%|███▎      | 20/61 [00:02<00:04,  8.63it/s]quant attn to fp8 ue8m0:  33%|███▎      | 20/61 [00:02<00:04,  8.62it/s]quant attn to fp8 ue8m0:  33%|███▎      | 20/61 [00:02<00:04,  8.62it/s]quant attn to fp8 ue8m0:  33%|███▎      | 20/61 [00:02<00:04,  8.62it/s]quant attn to fp8 ue8m0:  34%|███▍      | 21/61 [00:02<00:04,  8.50it/s]quant attn to fp8 ue8m0:  34%|███▍      | 21/61 [00:02<00:04,  8.51it/s]quant attn to fp8 ue8m0:  34%|███▍      | 21/61 [00:02<00:04,  8.50it/s]quant attn to fp8 ue8m0:  34%|███▍      | 21/61 [00:02<00:04,  8.49it/s]quant attn to fp8 ue8m0:  36%|███▌      | 22/61 [00:02<00:04,  8.71it/s]quant attn to fp8 ue8m0:  36%|███▌      | 22/61 [00:02<00:04,  8.70it/s]quant attn to fp8 ue8m0:  36%|███▌      | 22/61 [00:02<00:04,  8.70it/s]quant attn to fp8 ue8m0:  36%|███▌      | 22/61 [00:02<00:04,  8.69it/s]quant attn to fp8 ue8m0:  38%|███▊      | 23/61 [00:02<00:04,  8.83it/s]quant attn to fp8 ue8m0:  38%|███▊      | 23/61 [00:02<00:04,  8.83it/s]quant attn to fp8 ue8m0:  38%|███▊      | 23/61 [00:02<00:04,  8.82it/s]quant attn to fp8 ue8m0:  38%|███▊      | 23/61 [00:02<00:04,  8.83it/s]quant attn to fp8 ue8m0:  39%|███▉      | 24/61 [00:02<00:04,  9.04it/s]quant attn to fp8 ue8m0:  39%|███▉      | 24/61 [00:02<00:04,  9.04it/s]quant attn to fp8 ue8m0:  39%|███▉      | 24/61 [00:02<00:04,  9.03it/s]quant attn to fp8 ue8m0:  39%|███▉      | 24/61 [00:02<00:04,  9.03it/s]quant attn to fp8 ue8m0:  41%|████      | 25/61 [00:02<00:04,  8.51it/s]quant attn to fp8 ue8m0:  41%|████      | 25/61 [00:02<00:04,  8.50it/s]quant attn to fp8 ue8m0:  41%|████      | 25/61 [00:02<00:04,  8.50it/s]quant attn to fp8 ue8m0:  41%|████      | 25/61 [00:02<00:04,  8.50it/s]quant attn to fp8 ue8m0:  43%|████▎     | 26/61 [00:03<00:04,  8.34it/s]quant attn to fp8 ue8m0:  43%|████▎     | 26/61 [00:03<00:04,  8.33it/s]quant attn to fp8 ue8m0:  43%|████▎     | 26/61 [00:03<00:04,  8.33it/s]quant attn to fp8 ue8m0:  43%|████▎     | 26/61 [00:03<00:04,  8.33it/s]quant attn to fp8 ue8m0:  44%|████▍     | 27/61 [00:03<00:03,  8.57it/s]quant attn to fp8 ue8m0:  44%|████▍     | 27/61 [00:03<00:03,  8.57it/s]quant attn to fp8 ue8m0:  44%|████▍     | 27/61 [00:03<00:03,  8.57it/s]quant attn to fp8 ue8m0:  44%|████▍     | 27/61 [00:03<00:03,  8.57it/s]quant attn to fp8 ue8m0:  46%|████▌     | 28/61 [00:03<00:03,  8.80it/s]quant attn to fp8 ue8m0:  46%|████▌     | 28/61 [00:03<00:03,  8.80it/s]quant attn to fp8 ue8m0:  46%|████▌     | 28/61 [00:03<00:03,  8.79it/s]quant attn to fp8 ue8m0:  46%|████▌     | 28/61 [00:03<00:03,  8.79it/s]quant attn to fp8 ue8m0:  48%|████▊     | 29/61 [00:03<00:03,  8.60it/s]quant attn to fp8 ue8m0:  48%|████▊     | 29/61 [00:03<00:03,  8.60it/s]quant attn to fp8 ue8m0:  48%|████▊     | 29/61 [00:03<00:03,  8.60it/s]quant attn to fp8 ue8m0:  48%|████▊     | 29/61 [00:03<00:03,  8.59it/s]quant attn to fp8 ue8m0:  49%|████▉     | 30/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  49%|████▉     | 30/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  49%|████▉     | 30/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  49%|████▉     | 30/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  51%|█████     | 31/61 [00:03<00:03,  9.02it/s]quant attn to fp8 ue8m0:  51%|█████     | 31/61 [00:03<00:03,  9.02it/s]quant attn to fp8 ue8m0:  51%|█████     | 31/61 [00:03<00:03,  9.02it/s]quant attn to fp8 ue8m0:  51%|█████     | 31/61 [00:03<00:03,  9.02it/s]quant attn to fp8 ue8m0:  52%|█████▏    | 32/61 [00:03<00:03,  8.78it/s]quant attn to fp8 ue8m0:  52%|█████▏    | 32/61 [00:03<00:03,  8.78it/s]quant attn to fp8 ue8m0:  52%|█████▏    | 32/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  52%|█████▏    | 32/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  54%|█████▍    | 33/61 [00:03<00:03,  8.60it/s]quant attn to fp8 ue8m0:  54%|█████▍    | 33/61 [00:03<00:03,  8.61it/s]quant attn to fp8 ue8m0:  54%|█████▍    | 33/61 [00:03<00:03,  8.59it/s]quant attn to fp8 ue8m0:  54%|█████▍    | 33/61 [00:03<00:03,  8.59it/s]quant attn to fp8 ue8m0:  56%|█████▌    | 34/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  56%|█████▌    | 34/61 [00:03<00:03,  8.77it/s]quant attn to fp8 ue8m0:  56%|█████▌    | 34/61 [00:03<00:03,  8.76it/s]quant attn to fp8 ue8m0:  56%|█████▌    | 34/61 [00:03<00:03,  8.75it/s]quant attn to fp8 ue8m0:  57%|█████▋    | 35/61 [00:04<00:02,  8.71it/s]quant attn to fp8 ue8m0:  57%|█████▋    | 35/61 [00:04<00:02,  8.71it/s]quant attn to fp8 ue8m0:  57%|█████▋    | 35/61 [00:04<00:02,  8.71it/s]quant attn to fp8 ue8m0:  57%|█████▋    | 35/61 [00:04<00:02,  8.71it/s]quant attn to fp8 ue8m0:  59%|█████▉    | 36/61 [00:04<00:02,  8.58it/s]quant attn to fp8 ue8m0:  59%|█████▉    | 36/61 [00:04<00:02,  8.57it/s]quant attn to fp8 ue8m0:  59%|█████▉    | 36/61 [00:04<00:02,  8.57it/s]quant attn to fp8 ue8m0:  59%|█████▉    | 36/61 [00:04<00:02,  8.57it/s]quant attn to fp8 ue8m0:  61%|██████    | 37/61 [00:04<00:02,  8.44it/s]quant attn to fp8 ue8m0:  61%|██████    | 37/61 [00:04<00:02,  8.43it/s]quant attn to fp8 ue8m0:  61%|██████    | 37/61 [00:04<00:02,  8.44it/s]quant attn to fp8 ue8m0:  61%|██████    | 37/61 [00:04<00:02,  8.43it/s]quant attn to fp8 ue8m0:  62%|██████▏   | 38/61 [00:04<00:02,  8.57it/s]quant attn to fp8 ue8m0:  62%|██████▏   | 38/61 [00:04<00:02,  8.56it/s]quant attn to fp8 ue8m0:  62%|██████▏   | 38/61 [00:04<00:02,  8.57it/s]quant attn to fp8 ue8m0:  62%|██████▏   | 38/61 [00:04<00:02,  8.57it/s]quant attn to fp8 ue8m0:  64%|██████▍   | 39/61 [00:04<00:02,  8.12it/s]quant attn to fp8 ue8m0:  64%|██████▍   | 39/61 [00:04<00:02,  8.12it/s]quant attn to fp8 ue8m0:  64%|██████▍   | 39/61 [00:04<00:02,  8.12it/s]quant attn to fp8 ue8m0:  64%|██████▍   | 39/61 [00:04<00:02,  8.11it/s]quant attn to fp8 ue8m0:  66%|██████▌   | 40/61 [00:04<00:02,  8.13it/s]quant attn to fp8 ue8m0:  66%|██████▌   | 40/61 [00:04<00:02,  8.13it/s]quant attn to fp8 ue8m0:  66%|██████▌   | 40/61 [00:04<00:02,  8.12it/s]quant attn to fp8 ue8m0:  66%|██████▌   | 40/61 [00:04<00:02,  8.12it/s]quant attn to fp8 ue8m0:  67%|██████▋   | 41/61 [00:04<00:02,  7.95it/s]quant attn to fp8 ue8m0:  67%|██████▋   | 41/61 [00:04<00:02,  7.95it/s]quant attn to fp8 ue8m0:  67%|██████▋   | 41/61 [00:04<00:02,  7.95it/s]quant attn to fp8 ue8m0:  67%|██████▋   | 41/61 [00:04<00:02,  7.94it/s]quant attn to fp8 ue8m0:  69%|██████▉   | 42/61 [00:04<00:02,  8.18it/s]quant attn to fp8 ue8m0:  69%|██████▉   | 42/61 [00:04<00:02,  8.18it/s]quant attn to fp8 ue8m0:  69%|██████▉   | 42/61 [00:04<00:02,  8.18it/s]quant attn to fp8 ue8m0:  69%|██████▉   | 42/61 [00:04<00:02,  8.16it/s]quant attn to fp8 ue8m0:  70%|███████   | 43/61 [00:05<00:02,  8.16it/s]quant attn to fp8 ue8m0:  70%|███████   | 43/61 [00:05<00:02,  8.15it/s]quant attn to fp8 ue8m0:  70%|███████   | 43/61 [00:05<00:02,  8.15it/s]quant attn to fp8 ue8m0:  70%|███████   | 43/61 [00:05<00:02,  8.14it/s]quant attn to fp8 ue8m0:  72%|███████▏  | 44/61 [00:05<00:02,  8.43it/s]quant attn to fp8 ue8m0:  72%|███████▏  | 44/61 [00:05<00:02,  8.42it/s]quant attn to fp8 ue8m0:  72%|███████▏  | 44/61 [00:05<00:02,  8.42it/s]quant attn to fp8 ue8m0:  72%|███████▏  | 44/61 [00:05<00:02,  8.42it/s]quant attn to fp8 ue8m0:  74%|███████▍  | 45/61 [00:05<00:01,  8.76it/s]quant attn to fp8 ue8m0:  74%|███████▍  | 45/61 [00:05<00:01,  8.75it/s]quant attn to fp8 ue8m0:  74%|███████▍  | 45/61 [00:05<00:01,  8.75it/s]quant attn to fp8 ue8m0:  74%|███████▍  | 45/61 [00:05<00:01,  8.75it/s]quant attn to fp8 ue8m0:  75%|███████▌  | 46/61 [00:05<00:01,  8.73it/s]quant attn to fp8 ue8m0:  75%|███████▌  | 46/61 [00:05<00:01,  8.73it/s]quant attn to fp8 ue8m0:  75%|███████▌  | 46/61 [00:05<00:01,  8.73it/s]quant attn to fp8 ue8m0:  75%|███████▌  | 46/61 [00:05<00:01,  8.73it/s]quant attn to fp8 ue8m0:  77%|███████▋  | 47/61 [00:05<00:01,  8.95it/s]quant attn to fp8 ue8m0:  77%|███████▋  | 47/61 [00:05<00:01,  8.95it/s]quant attn to fp8 ue8m0:  77%|███████▋  | 47/61 [00:05<00:01,  8.94it/s]quant attn to fp8 ue8m0:  77%|███████▋  | 47/61 [00:05<00:01,  8.94it/s]quant attn to fp8 ue8m0:  79%|███████▊  | 48/61 [00:05<00:01,  8.80it/s]quant attn to fp8 ue8m0:  79%|███████▊  | 48/61 [00:05<00:01,  8.79it/s]quant attn to fp8 ue8m0:  79%|███████▊  | 48/61 [00:05<00:01,  8.79it/s]quant attn to fp8 ue8m0:  79%|███████▊  | 48/61 [00:05<00:01,  8.79it/s]quant attn to fp8 ue8m0:  80%|████████  | 49/61 [00:05<00:01,  9.02it/s]quant attn to fp8 ue8m0:  80%|████████  | 49/61 [00:05<00:01,  9.01it/s]quant attn to fp8 ue8m0:  80%|████████  | 49/61 [00:05<00:01,  9.01it/s]quant attn to fp8 ue8m0:  80%|████████  | 49/61 [00:05<00:01,  9.00it/s]quant attn to fp8 ue8m0:  82%|████████▏ | 50/61 [00:05<00:01,  8.83it/s]quant attn to fp8 ue8m0:  82%|████████▏ | 50/61 [00:05<00:01,  8.83it/s]quant attn to fp8 ue8m0:  82%|████████▏ | 50/61 [00:05<00:01,  8.82it/s]quant attn to fp8 ue8m0:  82%|████████▏ | 50/61 [00:05<00:01,  8.82it/s]quant attn to fp8 ue8m0:  84%|████████▎ | 51/61 [00:05<00:01,  8.44it/s]quant attn to fp8 ue8m0:  84%|████████▎ | 51/61 [00:05<00:01,  8.44it/s]quant attn to fp8 ue8m0:  84%|████████▎ | 51/61 [00:05<00:01,  8.44it/s]quant attn to fp8 ue8m0:  84%|████████▎ | 51/61 [00:05<00:01,  8.44it/s]quant attn to fp8 ue8m0:  85%|████████▌ | 52/61 [00:06<00:01,  8.21it/s]quant attn to fp8 ue8m0:  85%|████████▌ | 52/61 [00:06<00:01,  8.21it/s]quant attn to fp8 ue8m0:  85%|████████▌ | 52/61 [00:06<00:01,  8.21it/s]quant attn to fp8 ue8m0:  85%|████████▌ | 52/61 [00:06<00:01,  8.21it/s]quant attn to fp8 ue8m0:  87%|████████▋ | 53/61 [00:06<00:00,  8.40it/s]quant attn to fp8 ue8m0:  87%|████████▋ | 53/61 [00:06<00:00,  8.40it/s]quant attn to fp8 ue8m0:  87%|████████▋ | 53/61 [00:06<00:00,  8.39it/s]quant attn to fp8 ue8m0:  87%|████████▋ | 53/61 [00:06<00:00,  8.39it/s]quant attn to fp8 ue8m0:  89%|████████▊ | 54/61 [00:06<00:00,  8.45it/s]quant attn to fp8 ue8m0:  89%|████████▊ | 54/61 [00:06<00:00,  8.45it/s]quant attn to fp8 ue8m0:  89%|████████▊ | 54/61 [00:06<00:00,  8.44it/s]quant attn to fp8 ue8m0:  89%|████████▊ | 54/61 [00:06<00:00,  8.44it/s]quant attn to fp8 ue8m0:  90%|█████████ | 55/61 [00:06<00:00,  8.52it/s]quant attn to fp8 ue8m0:  90%|█████████ | 55/61 [00:06<00:00,  8.52it/s]quant attn to fp8 ue8m0:  90%|█████████ | 55/61 [00:06<00:00,  8.52it/s]quant attn to fp8 ue8m0:  90%|█████████ | 55/61 [00:06<00:00,  8.52it/s]quant attn to fp8 ue8m0:  92%|█████████▏| 56/61 [00:06<00:00,  8.34it/s]quant attn to fp8 ue8m0:  92%|█████████▏| 56/61 [00:06<00:00,  8.33it/s]quant attn to fp8 ue8m0:  92%|█████████▏| 56/61 [00:06<00:00,  8.31it/s]quant attn to fp8 ue8m0:  92%|█████████▏| 56/61 [00:06<00:00,  8.31it/s]quant attn to fp8 ue8m0:  93%|█████████▎| 57/61 [00:06<00:00,  8.10it/s]quant attn to fp8 ue8m0:  93%|█████████▎| 57/61 [00:06<00:00,  8.10it/s]quant attn to fp8 ue8m0:  93%|█████████▎| 57/61 [00:06<00:00,  8.08it/s]quant attn to fp8 ue8m0:  93%|█████████▎| 57/61 [00:06<00:00,  8.09it/s]quant attn to fp8 ue8m0:  95%|█████████▌| 58/61 [00:06<00:00,  7.94it/s]quant attn to fp8 ue8m0:  95%|█████████▌| 58/61 [00:06<00:00,  7.93it/s]quant attn to fp8 ue8m0:  95%|█████████▌| 58/61 [00:06<00:00,  7.92it/s]quant attn to fp8 ue8m0:  95%|█████████▌| 58/61 [00:06<00:00,  7.92it/s]quant attn to fp8 ue8m0:  97%|█████████▋| 59/61 [00:06<00:00,  8.11it/s]quant attn to fp8 ue8m0:  97%|█████████▋| 59/61 [00:06<00:00,  8.10it/s]quant attn to fp8 ue8m0:  97%|█████████▋| 59/61 [00:06<00:00,  8.11it/s]quant attn to fp8 ue8m0:  97%|█████████▋| 59/61 [00:06<00:00,  8.10it/s]quant attn to fp8 ue8m0:  98%|█████████▊| 60/61 [00:07<00:00,  8.39it/s]quant attn to fp8 ue8m0:  98%|█████████▊| 60/61 [00:07<00:00,  8.39it/s]quant attn to fp8 ue8m0:  98%|█████████▊| 60/61 [00:07<00:00,  8.39it/s]quant attn to fp8 ue8m0:  98%|█████████▊| 60/61 [00:07<00:00,  8.39it/s]quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.78it/s]quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.53it/s]
quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.77it/s]quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.77it/s]quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.52it/s]
quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.77it/s]quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.52it/s]
quant attn to fp8 ue8m0: 100%|██████████| 61/61 [00:07<00:00,  8.52it/s]
[2025-11-30 09:17:22 DP44 TP44 EP44] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!
[2025-11-30 09:17:22 DP44 TP44 EP44] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=146.00 GB, mem usage=36.18 GB.
[2025-11-30 09:17:22 DP45 TP45 EP45] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!
[2025-11-30 09:17:22 DP45 TP45 EP45] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=146.00 GB, mem usage=36.19 GB.
[2025-11-30 09:17:23 DP47 TP47 EP47] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!
[2025-11-30 09:17:23 DP47 TP47 EP47] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=146.00 GB, mem usage=36.19 GB.
[2025-11-30 09:17:23 DP46 TP46 EP46] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!
[2025-11-30 09:17:23 DP46 TP46 EP46] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=146.00 GB, mem usage=36.19 GB.
[2025-11-30 09:17:27 DP45 TP45 EP45] KV Cache is allocated. #tokens: 3122368, KV size: 102.18 GB
[2025-11-30 09:17:27 DP44 TP44 EP44] KV Cache is allocated. #tokens: 3122368, KV size: 102.18 GB
[2025-11-30 09:17:27 DP44 TP44 EP44] Memory pool end. avail mem=43.00 GB
[2025-11-30 09:17:27 DP45 TP45 EP45] Memory pool end. avail mem=43.00 GB
[2025-11-30 09:17:27 DP47 TP47 EP47] KV Cache is allocated. #tokens: 3122368, KV size: 102.18 GB
[2025-11-30 09:17:27 DP47 TP47 EP47] Memory pool end. avail mem=43.00 GB
[2025-11-30 09:17:27 DP46 TP46 EP46] KV Cache is allocated. #tokens: 3122368, KV size: 102.18 GB
[2025-11-30 09:17:27 DP46 TP46 EP46] Memory pool end. avail mem=43.00 GB
[2025-11-30 09:17:27 DP44 TP44 EP44] Capture cuda graph begin. This can take up to several minutes. avail mem=42.43 GB
[2025-11-30 09:17:27 DP46 TP46 EP46] Capture cuda graph begin. This can take up to several minutes. avail mem=42.43 GB
[2025-11-30 09:17:27 DP45 TP45 EP45] Capture cuda graph begin. This can take up to several minutes. avail mem=42.43 GB
[2025-11-30 09:17:27 DP47 TP47 EP47] Capture cuda graph begin. This can take up to several minutes. avail mem=42.43 GB
WARN: device mlx5_0 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_1 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_0 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_0 cannot allocate buffer on the specified memory type. Skipping...

WARN: WARN: device mlx5_4 cannot allocate buffer on the specified memory type. Skipping...
device mlx5_0 cannot allocate buffer on the specified memory type. Skipping...


WARN: device mlx5_1 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_1 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_1 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_5 cannot allocate buffer on the specified memory type. Skipping...

/sgl-workspace/nvshmem/src/host/transport/transport.cpp:nvshmemi_transport_init:282: init failed for transport: IBGDA
WARN: device mlx5_4 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_4 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_4 cannot allocate buffer on the specified memory type. Skipping...

WARN: device mlx5_5 cannot allocate buffer on the specified memory type. Skipping...

/sgl-workspace/nvshmem/src/host/transport/transport.cpp:nvshmemi_transport_init:282: init failed for transport: IBGDA
WARN: device mlx5_5 cannot allocate buffer on the specified memory type. Skipping...

/sgl-workspace/nvshmem/src/host/transport/transport.cpp:nvshmemi_transport_init:282: init failed for transport: IBGDA
WARN: device mlx5_5 cannot allocate buffer on the specified memory type. Skipping...

/sgl-workspace/nvshmem/src/host/transport/transport.cpp:nvshmemi_transport_init:282: init failed for transport: IBGDA
[2025-11-30 09:17:39 DP47 TP47 EP47] Capture cuda graph end. Time elapsed: 12.48 s. mem usage=31.09 GB. avail mem=11.34 GB.
[2025-11-30 09:17:39 DP44 TP44 EP44] Capture cuda graph end. Time elapsed: 12.50 s. mem usage=31.11 GB. avail mem=11.32 GB.
[2025-11-30 09:17:39 DP46 TP46 EP46] Capture cuda graph end. Time elapsed: 12.50 s. mem usage=31.09 GB. avail mem=11.34 GB.
[2025-11-30 09:17:39 DP45 TP45 EP45] Capture cuda graph end. Time elapsed: 12.57 s. mem usage=31.09 GB. avail mem=11.34 GB.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1130 09:17:41.162153 183926 transfer_engine.cpp:493] Metrics reporting thread started (interval: 5s)
I1130 09:17:41.162140 181464 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.52.103.121 port: 12001
I1130 09:17:41.162365 181464 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.52.103.121:16103
I1130 09:17:41.162478 181464 transfer_engine.cpp:185] Auto-discovering topology...
I1130 09:17:41.163575 181464 transfer_engine.cpp:200] Topology discovery complete. Found 4 HCAs.
W1130 09:17:41.168069 181464 nvlink_transport.cpp:385] Memory region 0x882c0000 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168129 181464 nvlink_transport.cpp:385] Memory region 0x8a0bbec0 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168169 181464 nvlink_transport.cpp:385] Memory region 0x8a6bbf00 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168190 181464 nvlink_transport.cpp:385] Memory region 0x8acbbf40 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168210 181464 nvlink_transport.cpp:385] Memory region 0x8b2bbf80 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168226 181464 nvlink_transport.cpp:385] Memory region 0x8e2bbfc0 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168243 181464 nvlink_transport.cpp:385] Memory region 0x912bc000 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168257 181464 nvlink_transport.cpp:385] Memory region 0x918bc040 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.168272 181464 nvlink_transport.cpp:385] Memory region 0xffe3ebff0040 is not allocated by cuMemCreate, but it can be used as local buffer
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1130 09:17:41.203408 181463 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.52.103.121 port: 12001
I1130 09:17:41.203415 183933 transfer_engine.cpp:493] Metrics reporting thread started (interval: 5s)
I1130 09:17:41.203523 181463 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.52.103.121:16118
I1130 09:17:41.203657 181463 transfer_engine.cpp:185] Auto-discovering topology...
I1130 09:17:41.204782 181463 transfer_engine.cpp:200] Topology discovery complete. Found 4 HCAs.
W1130 09:17:41.209399 181463 nvlink_transport.cpp:385] Memory region 0x67e4a900 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209453 181463 nvlink_transport.cpp:385] Memory region 0x66e89980 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209472 181463 nvlink_transport.cpp:385] Memory region 0x75ee26c0 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209489 181463 nvlink_transport.cpp:385] Memory region 0x764e2740 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209507 181463 nvlink_transport.cpp:385] Memory region 0xfff8bcfd0040 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209522 181463 nvlink_transport.cpp:385] Memory region 0xfff8a0fe0040 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209537 181463 nvlink_transport.cpp:385] Memory region 0x76ae27c0 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209549 181463 nvlink_transport.cpp:385] Memory region 0x770e2840 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.209563 181463 nvlink_transport.cpp:385] Memory region 0xffe3cbff0040 is not allocated by cuMemCreate, but it can be used as local buffer
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1130 09:17:41.229001 181462 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.52.103.121 port: 12001
I1130 09:17:41.229007 183939 transfer_engine.cpp:493] Metrics reporting thread started (interval: 5s)
I1130 09:17:41.229116 181462 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.52.103.121:16164
I1130 09:17:41.229295 181462 transfer_engine.cpp:185] Auto-discovering topology...
I1130 09:17:41.230366 181462 transfer_engine.cpp:200] Topology discovery complete. Found 4 HCAs.
W1130 09:17:41.234443 181462 nvlink_transport.cpp:385] Memory region 0x671bfe00 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234486 181462 nvlink_transport.cpp:385] Memory region 0x8add0080 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234498 181462 nvlink_transport.cpp:385] Memory region 0x8b3d0100 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234508 181462 nvlink_transport.cpp:385] Memory region 0x8b9d0180 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234516 181462 nvlink_transport.cpp:385] Memory region 0x8bfd0200 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234526 181462 nvlink_transport.cpp:385] Memory region 0x8efd0280 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234536 181462 nvlink_transport.cpp:385] Memory region 0x91fd0300 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234546 181462 nvlink_transport.cpp:385] Memory region 0x925d0380 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.234556 181462 nvlink_transport.cpp:385] Memory region 0xffe3ebff0040 is not allocated by cuMemCreate, but it can be used as local buffer
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1130 09:17:41.241878 181461 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.52.103.121 port: 12001
I1130 09:17:41.241891 183945 transfer_engine.cpp:493] Metrics reporting thread started (interval: 5s)
I1130 09:17:41.241964 181461 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.52.103.121:16614
I1130 09:17:41.242116 181461 transfer_engine.cpp:185] Auto-discovering topology...
I1130 09:17:41.242967 181461 transfer_engine.cpp:200] Topology discovery complete. Found 4 HCAs.
W1130 09:17:41.246934 181461 nvlink_transport.cpp:385] Memory region 0x51142500 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.246973 181461 nvlink_transport.cpp:385] Memory region 0x57152900 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.246985 181461 nvlink_transport.cpp:385] Memory region 0x57752940 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.246995 181461 nvlink_transport.cpp:385] Memory region 0x57d52980 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.247004 181461 nvlink_transport.cpp:385] Memory region 0x5d3e0d40 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.247014 181461 nvlink_transport.cpp:385] Memory region 0x603e0d80 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.247025 181461 nvlink_transport.cpp:385] Memory region 0x583529c0 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.247033 181461 nvlink_transport.cpp:385] Memory region 0x58952a00 is not allocated by cuMemCreate, but it can be used as local buffer
W1130 09:17:41.247042 181461 nvlink_transport.cpp:385] Memory region 0xffe3ebff0040 is not allocated by cuMemCreate, but it can be used as local buffer
[2025-11-30 09:17:41] Dummy health check server started in background thread at 0.0.0.0:30000
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.29, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.29, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.29, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.29, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.44, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.43, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.43, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.39, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.72, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.72, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.70, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.71, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.75, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.75, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.74, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.73, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.71, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.67, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.67, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.68, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.71, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.69, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.69, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 64, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 27.70, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 4.91, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 4.91, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 4.89, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 4.87, #queue-req: 0, 
[2025-11-30 09:17:44 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 117.38, #queue-req: 0, 
[2025-11-30 09:17:44 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 140.94, #queue-req: 0, 
[2025-11-30 09:17:44 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 126.99, #queue-req: 0, 
[2025-11-30 09:17:44 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 115.75, #queue-req: 0, 
[2025-11-30 09:18:51 DP47 TP47 EP47] Cache flushed successfully!
[2025-11-30 09:18:51 DP46 TP46 EP46] Cache flushed successfully!
[2025-11-30 09:18:51 DP45 TP45 EP45] Cache flushed successfully!
[2025-11-30 09:18:51 DP44 TP44 EP44] Cache flushed successfully!
[2025-11-30 09:19:01 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 18432, token usage: 0.01, pre-allocated usage: 0.01, #prealloc-req: 0, #transfer-req: 18, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.01, #queue-req: 0, 
[2025-11-30 09:19:03 DP45 TP45 EP45] Decode batch, #running-req: 2, #token: 19456, token usage: 0.01, pre-allocated usage: 0.01, #prealloc-req: 0, #transfer-req: 19, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.03, #queue-req: 0, 
[2025-11-30 09:19:03 DP46 TP46 EP46] Decode batch, #running-req: 7, #token: 11264, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 11, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.09, #queue-req: 0, 
[2025-11-30 09:19:04 DP47 TP47 EP47] Decode batch, #running-req: 2, #token: 30720, token usage: 0.01, pre-allocated usage: 0.01, #prealloc-req: 0, #transfer-req: 25, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.03, #queue-req: 0, 
[2025-11-30 09:19:06 DP47 TP47 EP47] Decode batch, #running-req: 5, #token: 25600, token usage: 0.01, pre-allocated usage: 0.01, #prealloc-req: 0, #transfer-req: 22, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 3.08, #queue-req: 0, 
[2025-11-30 09:19:06 DP46 TP46 EP46] Decode batch, #running-req: 2, #token: 9216, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 7, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.62, #queue-req: 0, 
[2025-11-30 09:19:07 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 13312, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 11, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.17, #queue-req: 0, 
[2025-11-30 09:19:07 DP45 TP45 EP45] Decode batch, #running-req: 6, #token: 11264, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 10, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.50, #queue-req: 0, 
[2025-11-30 09:19:07 DP47 TP47 EP47] Decode batch, #running-req: 3, #token: 22528, token usage: 0.01, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 13, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.92, #queue-req: 0, 
[2025-11-30 09:19:08 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 6144, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 6, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.64, #queue-req: 0, 
[2025-11-30 09:19:08 DP44 TP44 EP44] Decode batch, #running-req: 2, #token: 11264, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 11, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 2.48, #queue-req: 0, 
[2025-11-30 09:19:08 DP47 TP47 EP47] Decode batch, #running-req: 9, #token: 13312, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 13, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 11.17, #queue-req: 0, 
[2025-11-30 09:19:08 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 10240, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 10, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.24, #queue-req: 0, 
[2025-11-30 09:19:08 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 9216, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 9, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 3.27, #queue-req: 0, 
[2025-11-30 09:19:08 DP46 TP46 EP46] Decode batch, #running-req: 1, #token: 5120, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 3, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 2.86, #queue-req: 0, 
[2025-11-30 09:19:09 DP44 TP44 EP44] Decode batch, #running-req: 2, #token: 9216, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 9, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.77, #queue-req: 0, 
[2025-11-30 09:19:09 DP46 TP46 EP46] Decode batch, #running-req: 2, #token: 3072, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 3, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 2.57, #queue-req: 0, 
[2025-11-30 09:19:09 DP47 TP47 EP47] Decode batch, #running-req: 2, #token: 11264, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 11, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.77, #queue-req: 0, 
[2025-11-30 09:19:09 DP45 TP45 EP45] Decode batch, #running-req: 2, #token: 4096, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 4, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 2.32, #queue-req: 0, 
[2025-11-30 09:19:09 DP46 TP46 EP46] Decode batch, #running-req: 3, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 20.73, #queue-req: 0, 
[2025-11-30 09:19:10 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 8192, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 6, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.46, #queue-req: 0, 
[2025-11-30 09:19:10 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 3072, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.46, #queue-req: 0, 
[2025-11-30 09:19:10 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 2048, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 7.75, #queue-req: 0, 
[2025-11-30 09:19:10 DP44 TP44 EP44] Decode batch, #running-req: 2, #token: 4096, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 4, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 12.08, #queue-req: 0, 
[2025-11-30 09:19:10 DP47 TP47 EP47] Decode batch, #running-req: 2, #token: 7168, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 7, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 2.35, #queue-req: 0, 
[2025-11-30 09:19:11 DP47 TP47 EP47] Decode batch, #running-req: 1, #token: 6144, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 6, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 1.00, #queue-req: 0, 
[2025-11-30 09:19:11 DP44 TP44 EP44] Decode batch, #running-req: 1, #token: 2048, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.80, #queue-req: 0, 
[2025-11-30 09:19:11 DP44 TP44 EP44] Decode batch, #running-req: 2, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 23.70, #queue-req: 0, 
[2025-11-30 09:19:11 DP45 TP45 EP45] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 0.75, #queue-req: 0, 
[2025-11-30 09:19:11 DP47 TP47 EP47] Decode batch, #running-req: 6, #token: 0, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.60, #queue-req: 0, 
